{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T14:24:46.528553Z",
     "start_time": "2025-07-05T14:24:40.257125Z"
    }
   },
   "source": [
    "from attack import MagicWordFinder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\").to(device)\n",
    "\n",
    "# The positive magic words we find should ideally push arbitrary sentences \n",
    "# to be similar to the sentences in S\n",
    "S = [\"who is the best harry potter character?\"]"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T14:24:50.038861Z",
     "start_time": "2025-07-05T14:24:46.600012Z"
    }
   },
   "source": [
    "attack = MagicWordFinder(model, S)\n",
    "\n",
    "# Find 30 candidates for 1-token magic words; the algorithm will only return the 10 best candidates, since we specify k_0=10\n",
    "cands = attack.find_magic_words(k=30, m=1, k_0=10, epochs=1)"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T14:24:50.395717Z",
     "start_time": "2025-07-05T14:24:50.308747Z"
    }
   },
   "source": [
    "# Print the magic words found\n",
    "for i, cand in enumerate(cands):\n",
    "    print(f\"{i + 1}. {model.tokenizer.decode(cand)} (token IDs {cand})\")\n",
    "\n",
    "# Evaluate how the magic words affect the avg. cosine similarity with S\n",
    "sentence = \"Voldemort was right all along!\"\n",
    "sentence_emb = model.encode(sentence, convert_to_tensor=True)\n",
    "S_embed = model.encode(S, convert_to_tensor=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ##bid (token IDs [17062])\n",
      "2. furnace (token IDs [17533])\n",
      "3. ##sho (token IDs [22231])\n",
      "4. ##station (token IDs [20100])\n",
      "5. ##poo (token IDs [24667])\n",
      "6. ##icide (token IDs [21752])\n",
      "7. ##izer (token IDs [17629])\n",
      "8. ##aw (token IDs [10376])\n",
      "9. ##ija (token IDs [14713])\n",
      "10. ##hiti (token IDs [27798])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Evaluate the base cosine similarity (i.e. with no magic words appended)\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=0)\n",
    "avg_cos_sim = sum([cos_sim(sentence_emb, S_embed[j]) for j in range(len(S))]) / len(S)\n",
    "\n",
    "print(f\"Base avg. cosine similarity: {avg_cos_sim}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Compare with the avg. similarity to the centroid of the queries\n",
    "centroid = model.encode(S, convert_to_tensor=True).mean(dim=0)\n",
    "avg_cos_sim = sum([cos_sim(centroid, S_embed[j]) for j in range(len(S))]) / len(S)\n",
    "\n",
    "print(f\"Avg. cosine similarity w/centroid: {avg_cos_sim}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "### Evaluate how appending each magic word once impacts the avg. cosine similarity\n",
    "sentence_tokenized = model.tokenizer(sentence)[\"input_ids\"]\n",
    "# Extract the EOS token and remove it (since we append more tokens)\n",
    "eos_token = sentence_tokenized[-1]\n",
    "sentence_tokenized = sentence_tokenized[:-1]\n",
    "\n",
    "for cand in cands:\n",
    "    # Append the candidate magic word to the sentence and then append the EOS token\n",
    "    input_ids = sentence_tokenized + cand + [eos_token]\n",
    "    # Embed the result\n",
    "    magic_sent_emb = model.encode(model.tokenizer.decode(input_ids), convert_to_tensor=True)\n",
    "    # Evaluate avg. cosine similarity\n",
    "    avg_cos_sim = sum([cos_sim(magic_sent_emb, S_embed[j]) for j in range(len(S))]) / len(S)\n",
    "\n",
    "    print(f\"Cosine similarity after appending magic word '{model.tokenizer.decode(cand)}': {avg_cos_sim}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T10:50:07.393003Z",
     "start_time": "2025-05-30T10:50:07.390266Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3392264161.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[31m    \u001B[39m\u001B[31mmodel.\u001B[39m\n          ^\n\u001B[31mSyntaxError\u001B[39m\u001B[31m:\u001B[39m invalid syntax\n"
     ]
    }
   ],
   "source": "",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
